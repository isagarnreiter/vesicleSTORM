#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
@author: isabellegarnreiter

This code uses the output from filter_STORMdata.py and creates a dataframe, where each row corresponds to data from one cluster, identified in the 647nm channel. 

Current rows:
'FileName': filename of the acquisition
'Date': date at which the acquisitopn was made
'647nm': Marker in the 647nm channel
'680nm': marker in the 680nm channel
'DIV': number of days in vitro
'Acquisition': acqusition number 
'ROI label': Roi number 
'ROI': bounding box of 3D ROI mask
'points': list of points in the cluster
'centroid': centroid coordinates of the point cloud
'NN_647_to_680': list of nearest neighbour distances for each point in the 647nm channel to points in the 680nmchannel
'NN_647_to_680_ind': indexes of selected points in the 680nm channels from the original input list
'NN_647_to_647': list of nearest neighbour distances between points in the 647nm channel
'centroid_dist_647_to_680': nearest neighbour distance from the centroid to the 680nm channel
'volume': volume of the cluster - calculated from the mask
'spherecity': spherecity of the cluster - calculated from the mask

"""


import numpy as np
import pandas as pd
from glob import glob
import functions as fcts
from scipy.spatial import KDTree
import os
import re


#target_dir =  filedialog.askdirectory()
target_dir = '/Users/isabellegarnreiter/Desktop/PSD'
filename = 'PSDstorm_data' 

#name of the widefield image in each folder. 
widefield_image = ''

#folder path in which to find the demixed storm files.
glob_folder_pattern = '*/CellZone*/*emix'

#what to match the files by to create the dataframe. If you want to use all the files in the directory, leave as an empty string.
match = ''

#update lists for markers and DIVs
markers = ['SPON647', 'DEP647', 'PSD680', 'Bassoon680', 'VAMP2680', 'VGLUT647', 'rimbp680', 'Basson647', 'SPON680', 'DEP680']
DIVs = ['6DIV', '8DIV', '10DIV', '13DIV']


params = fcts.get_default_params()
params['647_channel'] = 'channel2'

storm_data = pd.DataFrame(columns = ['FileName', 
                                     'Date', 
                                     '647nm', 
                                     '680nm', 
                                     'DIV', 
                                     'Acquisition', 
                                     'ROI label', 
                                     'ROI', 
                                     'points_647', 
                                     'points_680',
                                     'centroid',
                                     'NN_647_to_680', 
                                     'NN_647_to_680_ind',
                                     'NN_647_to_647',
                                     'centroid_dist_647_to_680',
                                     'volume', 
                                     'spherecity'])



# usable_exp = pd.read_csv('/users/isabellegarnreiter/documents/vesicleSTORM/data/STORM_binary_list.csv',encoding='latin', sep=',').to_numpy()
# filename  = usable_exp[:,0]+'_'+usable_exp[:,1]
# files_infos = dict(zip(filename, usable_exp[:,2:]))


date_pattern = re.compile(r'^\d')
        
i = 0
for folder in os.listdir(target_dir):
    folder_path = os.path.join(target_dir, folder)
    # Check if folder is a directory and if the name starts with a date
    if os.path.isdir(folder_path) and re.compile(r'^\d').match(folder) and match in folder:
        print(folder)

        Demix_folders = glob(folder_path + glob_folder_pattern)
        for demix in Demix_folders:
            if os.path.isdir(demix):
                #MAKE SURE YOU HAVE THE RIGHT CHANNEL HERE.
                if params['647_channel'] == 'channel1':
                    channel680 = glob(demix + '*/*w2*.csv')[0]

                elif params['647_channel'] == 'channel2':
                    channel680 = glob(demix + '*/*w1*.csv')[0]

            #load the dataset for the 680-marker, to measure the nearest neighbour to the points in the 647 channel
            data_in_680 = pd.read_csv(channel680)[['x [nm]', 'y [nm]', 'z [nm]']].to_numpy(dtype=np.float64)
            data_in_680[:,2] +=550

            #load the clusters, the points in the 680 channel and the points in the 647 channel
            points_647_file = os.path.join(demix, 'data/', 'points_647.npy')
            clusters_647_file = os.path.join(demix, 'data/', 'clusters_647.npy')
            points_647 = np.load(points_647_file, allow_pickle=True)[()]
            clusters_647 = np.load(clusters_647_file, allow_pickle=True)[()]
            points_680_file = os.path.join(demix, 'data/', 'points_680.npy')
            points_680 = np.load(points_680_file, allow_pickle=True)[()]

            Date = folder[:6]
            marker = [x for x in markers if x in folder]
            marker647 = [x for x in marker if '647' in x][0]
            marker680 = [x for x in marker if '680' in x][0]
            DIV = [x for x in DIVs if x in folder][0]
            acquisition = demix.split('/')[-2]

            for nb in points_647.keys():
                i=i+1
                ROI_label = int(nb)
                cluster_647 = clusters_647[nb]
                point_647 = points_647[nb]

                if params['filter_680']==True:
                    point_680 = points_680[nb]

                else:
                    point_680 = None

                if len(point_647) > 0:
                    data_680_dist = KDTree(data_in_680)
                    nn_647_680, nearest_ind_680 = data_680_dist.query(point_647, k=1) 
                
                    centroid = np.mean(point_647, axis=0)
                    nn_centroid647_680, nni_centroid647_680 = data_680_dist.query(centroid, k=1) 
                
                    self_dist = KDTree(point_647)        
                    nn_647_647, nearest_ind_647 = self_dist.query(point_647, k=15) 

                else:
                    #if there are not points in the 647 channel for a particular cluster, which can happen if the wfi is generated using points in both channels, 
                    #the cluster is not included in the dataframe.
                    break
                
                volume, sphericity = fcts.calculate_volume_sphericity(cluster_647, params)
                
                storm_data.loc[i] = [folder, 
                        Date, 
                        marker647, 
                        marker680, 
                        DIV, 
                        acquisition, 
                        ROI_label, 
                        cluster_647, 
                        point_647,
                        point_680, 
                        centroid,
                        nn_647_680, 
                        nearest_ind_680, 
                        nn_647_647,
                        nn_centroid647_680,
                        volume, 
                        sphericity]


storm_data['point count'] = storm_data['points_647'].apply(lambda arr:arr.shape[0])
storm_data['mean_coloc_680'] = storm_data['NN_647_to_680'].apply(lambda arr:arr.mean())
storm_data['stderror_coloc_680'] = storm_data['NN_647_to_680'].apply(lambda arr:arr.std())
storm_data['mean_coloc_647'] = storm_data['NN_647_to_647'].apply(lambda arr:arr.mean())
storm_data['stderror_coloc_647'] = storm_data['NN_647_to_647'].apply(lambda arr:arr.std())


storm_data.to_pickle(os.path.join(target_dir, f'{filename}.pkl'))  
