{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8961f317",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying DBScan\n",
    "#define intensity threshold. Points which have a lower intensity will be excluded from the clustering.\n",
    "int_threshold=0\n",
    "\n",
    "#apply the threshold on the vesicle and synaptic marker datasets\n",
    "vesicles_thresh = vesicles[vesicles[:,3]>int_threshold,0:3]\n",
    "synapse_marker_thresh = synapse_marker[synapse_marker[:,3]>int_threshold, 0:3]\n",
    "\n",
    "#compute the fraction of points including in the clustering.\n",
    "frct_points_included = vesicles_thresh.shape[0]/vesicles.shape[0]*100\n",
    "print('% points above threshold:', frct_points_included)\n",
    "\n",
    "# Normalize the data to have zero mean and unit variance\n",
    "data_norm = (vesicles_thresh - np.mean(vesicles_thresh, axis=0)) / np.std(vesicles_thresh, axis=0)\n",
    "\n",
    "# Apply DBSCAN to the normalized data\n",
    "dbscan = DBSCAN(eps=800, min_samples=150)\n",
    "labels = dbscan.fit_predict(vesicles_thresh)\n",
    "\n",
    "#compute the number of clusters rendered by DBSCAN\n",
    "unique_labels = np.unique(labels[labels >= 0])\n",
    "n_clusters = len(unique_labels)\n",
    "print('number of clusters:', n_clusters)\n",
    "\n",
    "plt.scatter(vesicles[:,0], vesicles[:,1], s=0.1, alpha=0.5)\n",
    "for label in unique_labels:\n",
    "    mask = labels == label\n",
    "    if np.any(mask):\n",
    "        plt.scatter(vesicles_thresh[mask, 0], vesicles_thresh[mask, 1], s=20)\n",
    "\n",
    "plt.title(\"DBSCAN Clusters\")\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a227fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_distances = {}\n",
    "for i in keywords:\n",
    "    for j in range(0, len(file_dic[i])):\n",
    "            min_dist = fcts.calc_distance_squared_two(file_dic[i][j], file_dic[i][j])\n",
    "            if i in min_distances:\n",
    "                min_distances[i] = np.append(min_distances[i], min_dist)\n",
    "            else:\n",
    "                min_distances[i] = np.array([min_dist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a60494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#point pattern analysis in 2D\n",
    "\n",
    "import pointpats\n",
    "coordinates=pd.DataFrame(vesicle_clusters['210404 SPON647_PSD680 10DIV_CellZone1'][1][:,0:2], columns = ['x', 'y'])\n",
    "\n",
    "g_test = pointpats.distance_statistics.g_test(coordinates, support=40, keep_simulations=True)\n",
    "\n",
    "f, ax = plt.subplots(\n",
    "    1, 2, figsize=(9, 3), gridspec_kw=dict(width_ratios=(6, 3))\n",
    ")\n",
    "# plot all the simulations with very fine lines\n",
    "ax[0].plot(\n",
    "    g_test.support, g_test.simulations.T, color=\"k\", alpha=0.01\n",
    ")\n",
    "# and show the average of simulations\n",
    "ax[0].plot(\n",
    "    g_test.support,\n",
    "    np.median(g_test.simulations, axis=0),\n",
    "    color=\"cyan\",\n",
    "    label=\"median simulation\",\n",
    ")\n",
    "\n",
    "\n",
    "# and the observed pattern's G function\n",
    "ax[0].plot(\n",
    "    g_test.support, g_test.statistic, label=\"observed\", color=\"red\"\n",
    ")\n",
    "\n",
    "# clean up labels and axes\n",
    "ax[0].set_xlabel(\"distance\")\n",
    "ax[0].set_ylabel(\"% of nearest neighbor\\ndistances shorter\")\n",
    "ax[0].legend()\n",
    "#ax[0].set_xlim(0, 2000)\n",
    "ax[0].set_title(r\"Ripley's $G(d)$ function\")\n",
    "\n",
    "# plot the pattern itself on the next frame\n",
    "ax[1].scatter(*coordinates)\n",
    "\n",
    "# and clean up labels and axes there, too\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_yticks([])\n",
    "ax[1].set_xticklabels([])\n",
    "ax[1].set_yticklabels([])\n",
    "ax[1].set_title(\"Pattern\")\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3750db44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#point density analysis in 2d\n",
    "\n",
    "# Set up figure and axis\n",
    "f, ax = plt.subplots(1, figsize=(6, 6))\n",
    "# Generate and add KDE with a shading of 50 gradients\n",
    "# coloured contours, 75% of transparency,\n",
    "# and the reverse viridis colormap\n",
    "seaborn.kdeplot(coordinates, x='x',y='y',\n",
    "    n_levels=50,\n",
    "    shade=True,\n",
    "    alpha=0.55,\n",
    "    cmap=\"viridis_r\",\n",
    ")\n",
    "\n",
    "# Remove axes\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d4f40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading a single file as example using outdated fct.\n",
    "\n",
    "ex_zone = ['/Volumes/STORM_Nathalie/STORM DeMixing/210414 DEP647_PSD680 8DIV/CellZone4/Demix/CoordTable_SAFE360_MULTIPLEXING_demixed_w1_UncertaintyFiltered.csv',\n",
    "  '/Volumes/STORM_Nathalie/STORM DeMixing/210414 DEP647_PSD680 8DIV/CellZone4/Demix/CoordTable_SAFE360_MULTIPLEXING_demixed_w2_UncertaintyFiltered.csv']\n",
    "\n",
    "ex_zone = list_of_files[0]\n",
    "\n",
    "vesicles = pd.read_csv(ex_zone[0])[['x [nm]', 'y [nm]', 'z [nm]']].to_numpy(dtype=np.float64)\n",
    "synapse_marker = pd.read_csv(ex_zone[1])[['x [nm]', 'y [nm]', 'z [nm]']].to_numpy(dtype=np.float64)\n",
    "\n",
    "data=vesicles\n",
    "image_size = (730,730,16)\n",
    "kernel_size = (50,50,1)\n",
    "sigma = 12\n",
    "\n",
    "wide_field_image = fcts.get_wide_field(vesicles, image_size, kernel_size, sigma)\n",
    "\n",
    "coords = np.array(coords)\n",
    "\n",
    "plt.imshow(image[:,:,0])\n",
    "plt.scatter(coords[:,1], coords[:,0], c='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e025c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#minimum distance for clusters of vesicles to PSD95 areas - but done as point location instead of point density\n",
    "\n",
    "image_size = (730,730,16)\n",
    "kernel_size = (50,50,1)\n",
    "sigma = 15\n",
    "\n",
    "max_threshold_ves = 12\n",
    "max_threshold_mark = 4\n",
    "max_area = (21,21,21)\n",
    "\n",
    "min_dist_marker = {}\n",
    "min_dist_vesicle = {}\n",
    "SNR_dict = {}\n",
    "for file_name in list_of_files:\n",
    "    \n",
    "    if access == 'drive':\n",
    "        new_file_name = f\"{(file_name[0]).split('/')[4]}_{(file_name[0]).split('/')[5]}\"\n",
    "    \n",
    "    elif access == 'computer':\n",
    "        new_file_name = f\"{(file_name[0]).split('/')[-1][0:-3]}\"\n",
    "    \n",
    "    print(new_file_name)\n",
    "    \n",
    "    file_info = files_infos[new_file_name]\n",
    "    \n",
    "    \n",
    "    if file_info[0] == 0:\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        vesicles = pd.read_csv(file_name[file_info[1]])[['x [nm]', 'y [nm]', 'z [nm]']].to_numpy(dtype=np.float64)\n",
    "        synapse_marker = pd.read_csv(file_name[(file_info[1]-1)**2])[['x [nm]', 'y [nm]', 'z [nm]']].to_numpy(dtype=np.float64)\n",
    "        \n",
    "        #plot the locations of both the vesicles and the synaptic marker using a large point spread function.\n",
    "        wide_field_vesicles = fcts.get_wide_field(vesicles, image_size, kernel_size, sigma)\n",
    "        wide_field_marker = fcts.get_wide_field(synapse_marker, image_size, kernel_size, sigma)\n",
    "        \n",
    "        #plot the locations of the vesicles using the same image size as above to a single pixel size\n",
    "        image_vesicles = fcts.get_wide_field(vesicles, image_size, (1,1,1), sigma)\n",
    "        \n",
    "        #calculate the intensity threshold for the large PSF images, depending on an arbitrary intensity threshold, dependent on the mean and std of each image.\n",
    "        ves_thresh = wide_field_vesicles.mean() + wide_field_vesicles.std() * max_threshold_ves\n",
    "        marker_thresh = wide_field_marker.mean() + wide_field_marker.std() * max_threshold_mark\n",
    "        \n",
    "        #create a mask of the large PSF images where for the pixels above the threshold\n",
    "        mask_vesicles = (wide_field_vesicles > ves_thresh) * 1\n",
    "\n",
    "        plt.imshow(mask_vesicles[:,:,0])\n",
    "        plt.show()\n",
    "        mask_marker = (wide_field_marker > marker_thresh) * 1\n",
    "\n",
    "        #create an array for the vesicles which are located within the mask\n",
    "        masked_vesicles = np.where(mask_vesicles, image_vesicles, 0)\n",
    "        vesicle_pos_tup = np.where(masked_vesicles > 0)\n",
    "        vesicle_pos = np.array([vesicle_pos_tup[0],vesicle_pos_tup[1],vesicle_pos_tup[2]]).T\n",
    "        vesicle_pos = vesicle_pos/image_size * 49660\n",
    "        \n",
    "        nb_vesicles_tot = vesicles.shape[0]\n",
    "        nb_vesicles_selected = vesicle_pos.shape[0]\n",
    "        snr = nb_vesicles_selected/nb_vesicles_tot\n",
    "        \n",
    "        #calculate the distance between vesicles within the masked images and all synapse markers\n",
    "        distance_to_marker = fcts.calc_distance_squared_two(vesicle_pos, synapse_marker)\n",
    "        distance_to_vesicles = fcts.calc_distance_squared_two(vesicle_pos, vesicle_pos)\n",
    "        print(distance_to_vesicles)\n",
    "        min_dist_marker[new_file_name] = distance_to_marker\n",
    "        min_dist_vesicle[new_file_name] = distance_to_vesicles\n",
    "        SNR_dict[new_file_name] = snr\n",
    "        \n",
    "        \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9820e2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def merge_points(points, diameter=40):\n",
    "    \"\"\"\n",
    "    Merges points that are within `diameter` distance of each other and returns a new array with one point per vesicle\n",
    "    \"\"\"\n",
    "    merged_points = []\n",
    "    while len(points) > 0:\n",
    "        p = points[0]\n",
    "        points = np.delete(points, 0, axis=0)\n",
    "        nearby_points = np.linalg.norm(points - p, axis=1) < diameter\n",
    "        while np.sum(nearby_points) > 0:\n",
    "            p = np.mean(np.concatenate([points[nearby_points], [p]]), axis=0)\n",
    "            points = np.delete(points, np.where(nearby_points)[0], axis=0)\n",
    "            nearby_points = np.linalg.norm(points - p, axis=1) < diameter\n",
    "        merged_points.append(p)\n",
    "    return np.array(merged_points)\n",
    "\n",
    "\n",
    "# Example dataset of 100 points with x,y,z coordinates\n",
    "points = np.random.rand(100, 2)\n",
    "\n",
    "# Merge points within 40nm of each other\n",
    "merged_points = merge_points(points, diameter=0.1)\n",
    "\n",
    "# Check the number of original points vs merged points\n",
    "print(f\"Original points: {points.shape[0]}, Merged points: {merged_points.shape[0]}\")\n",
    "\n",
    "plt.scatter(points[:,0], points[:,1], alpha=0.5)\n",
    "plt.scatter(merged_points[:,0], merged_points[:,1], alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7947dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#permutation test\n",
    "\n",
    "x = vesicle_type_nd_DIV['DEP647_PSD680 8DIV']\n",
    "y = vesicle_type_nd_DIV['SPON647_PSD680 8DIV']\n",
    "\n",
    "\n",
    "def statistic(x, y, axis):\n",
    "    return np.mean(x, axis=axis) - np.mean(y, axis=axis)\n",
    "\n",
    "from scipy.stats import permutation_test\n",
    "# because our statistic is vectorized, we pass `vectorized=True`\n",
    "# `n_resamples=np.inf` indicates that an exact test is to be performed\n",
    "res = permutation_test((x, y), statistic, vectorized=True,\n",
    "                       n_resamples=9999, alternative='less')\n",
    "print(res.statistic)\n",
    "print(res.pvalue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc3a4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unused functions\n",
    "\n",
    "\n",
    "@njit(fastmath=True,parallel=True)\n",
    "def calc_distance_squared_two(vec_1,vec_2):\n",
    "\n",
    "    res=np.empty((vec_1.shape[0]),dtype=vec_1.dtype)\n",
    "    for i in prange(vec_1.shape[0]):\n",
    "        dist= np.sqrt((vec_1[i,0]-vec_2[:,0])**2+(vec_1[i,1]-vec_2[:,1])**2+(vec_1[i,2]-vec_2[:,2])**2)\n",
    "        res[i] = np.min(dist[np.nonzero(dist)])\n",
    "    return res\n",
    "\n",
    "    \n",
    "def get_wide_field(data, image_size, kernel_size, sigma):\n",
    "    \n",
    "    #prep data for image\n",
    "    data[:,2] =+ 485\n",
    "    data = data/49660*image_size\n",
    "    \n",
    "    #initialise background with a buffer\n",
    "    buffered_image_dim = tuple(map(lambda i, j: i + j, image_size, kernel_size))\n",
    "    buffered_image = np.zeros(buffered_image_dim)\n",
    "\n",
    "    #initialise the gaussian distribution with size being the size of the kernel and sigma defining the standard deviation of the point spread function\n",
    "    kernel = np.fromfunction(lambda x, y, z : (1/(2*np.pi*sigma**2)) * np.exp((-1*((x-(kernel_size[0]-1)/2)**2+(y-(kernel_size[1]-1)/2)**2+(z-(kernel_size[2]-1)/2)**2)/(2*sigma**2))), kernel_size)\n",
    "    kernel = kernel / np.max(kernel)\n",
    "\n",
    "    #get the pixel location of the data\n",
    "    point_coord = np.round(data).astype(int)\n",
    "    #add gaussian point spread function at each point location\n",
    "    for y, x, z in point_coord:\n",
    "        buffered_image[x:x+kernel.shape[0], y:y+kernel.shape[1], z:z+kernel.shape[2]] += kernel\n",
    "    \n",
    "    #remove buffer from image\n",
    "    image = buffered_image[kernel.shape[0]//2:-kernel.shape[0]//2, kernel.shape[1]//2:-kernel.shape[1]//2, kernel.shape[2]//2:-kernel.shape[2]//2]\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "def get_local_max_3d(img, kernelsize, std_threshold):\n",
    "    # Get coordinates of local maxima \n",
    "\n",
    "    img2 = ndimage.maximum_filter(img, size=kernelsize)\n",
    "\n",
    "    # Threshold the image to find locations of interest\n",
    "    img_thresh = img2.mean() + img2.std() * std_threshold\n",
    "\n",
    "    # Since we're looking for maxima find areas greater than img_thresh\n",
    "    labels, num_labels = ndimage.label(img2 > img_thresh)\n",
    "\n",
    "    # Get the positions of the maxima\n",
    "    coords = ndimage.center_of_mass(img, labels=labels, index=np.arange(1, num_labels + 1))\n",
    "    coords = np.array(coords)\n",
    "    # Get the maximum value in the labels    \n",
    "\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e43b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_array(arr):\n",
    "    min_val = np.min(arr)\n",
    "    max_val = np.max(arr)\n",
    "    return (arr - min_val) / (max_val - min_val)\n",
    "\n",
    "# apply the normalization function to the column of arrays\n",
    "storm_data['nn_680_normalized'] = storm_data['nearest_neighbor_680'].apply(normalize_array)\n",
    "\n",
    "\n",
    "# concatenate the normalized arrays into a single numpy array\n",
    "dep_array = np.concatenate(storm_data[storm_data['647nm']=='DEP647']['nearest_neighbor_680'].to_numpy())\n",
    "spon_array = np.concatenate(storm_data[storm_data['647nm']=='SPON647']['nearest_neighbor_680'].to_numpy())\n",
    "\n",
    "n, x, _ = plt.hist(dep_array, bins=200, \n",
    "                   histtype=u'step', density=True)  \n",
    "\n",
    "density_dep = sc.gaussian_kde(dep_array)\n",
    "density_spon = sc.gaussian_kde(spon_array)\n",
    "\n",
    "\n",
    "plt.plot(x, density_dep(x), alpha=0.5)\n",
    "plt.plot(x, density_spon(x), alpha=0.5)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "dep_array = np.sort(dep_array)\n",
    "spon_array = np.sort(spon_array)\n",
    "\n",
    "dep_array_sorted = dep_array[:-1].reshape((8, dep_array[:-1].shape[0]//8))\n",
    "dep_array_sorted_mean = np.mean(dep_array_sorted, axis=1)\n",
    "spon_array_sorted = spon_array[:-1].reshape((8, spon_array[:-1].shape[0]//8))\n",
    "spon_array_sorted_mean = np.mean(spon_array_sorted, axis=1)\n",
    "\n",
    "plt.scatter(x=[np.zeros(8), np.ones(8)], y = [spon_array_sorted_mean, dep_array_sorted_mean])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d45124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_array = storm_data[storm_data['647nm']=='DEP647']['nearest_neighbor_680'].to_numpy()\n",
    "spon_array = storm_data[storm_data['647nm']=='SPON647']['nearest_neighbor_680'].to_numpy()\n",
    "    \n",
    "quarter_len = storm_data['nearest_neighbor_680'].apply(lambda x: len(x) // 4)\n",
    "quarter_len = quarter_len.to_numpy()\n",
    "# separate the array into quarters and calculate the mean of each quarter\n",
    "quarter_means = pd.DataFrame()\n",
    "for i in range(4):\n",
    "    quarter_arr = storm_data['nearest_neighbor_680'].apply(lambda x: x[i*quarter_len:(i+1)*quarter_len])\n",
    "    quarter_mean = quarter_arr.apply(lambda x: np.mean(x))\n",
    "    quarter_means[f'quarter_{i+1}_mean'] = quarter_mean\n",
    "\n",
    "print(quarter_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47819c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the normalized arrays into a single numpy array\n",
    "dep_array_norm = np.concatenate(storm_data[storm_data['647nm']=='DEP647']['nn_680_normalized'].to_numpy())\n",
    "spon_array_norm = np.concatenate(storm_data[storm_data['647nm']=='SPON647']['nn_680_normalized'].to_numpy())\n",
    "\n",
    "n, x, _ = plt.hist(dep_array, bins=50, \n",
    "                   histtype=u'step', density=True)  \n",
    "\n",
    "\n",
    "density_dep = sc.gaussian_kde(dep_array_norm)\n",
    "density_spon = sc.gaussian_kde(dep_array_norm)\n",
    "\n",
    "\n",
    "plt.plot(x, density_dep(x))\n",
    "plt.plot(x, density_spon(x))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b3f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the n shortest distances from one vesicles to its n neighbors for each vesicle\n",
    "\n",
    "n = 50\n",
    "\n",
    "all_points = storm_data['points'].to_numpy()\n",
    "\n",
    "NNDs = np.array([])\n",
    "for points in all_points:\n",
    "    tree = KDTree(points)\n",
    "    nearest_dist, nearest_ind = tree.query(points, k=n+1) \n",
    "    NNDs = np.concatenate((NNDs, nearest_dist), axis=None)\n",
    "\n",
    "NNDs = NNDs.reshape((int(NNDs.shape[0]/(n+1)), n+1))\n",
    "NNDs.shape\n",
    "NNDs_df  = pd.DataFrame(NNDs, columns = np.array(range(0,n+1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923fff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a scatter plot\n",
    "#plt.scatter(NNDs_df.columns.values, NNDs_df.mean(axis=0))\n",
    "plt.plot(np.unique(NNDs_df.columns.values), np.poly1d(np.polyfit(NNDs_df.columns.values, NNDs_df.mean(axis=0), 1))(np.unique(NNDs_df.columns.values)))\n",
    "\n",
    "# Set the  -axis label\n",
    "plt.xlabel('Nearest Neighbor Index')\n",
    "\n",
    "# Set the y-axis label\n",
    "plt.ylabel('Mean Nearest Neighbor Distance')\n",
    "\n",
    "# Set the plot title\n",
    "plt.title('Nearest Neighbor Distance')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68db3a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying to look at the intervesicular distance for the nth order of nearest neighbour. very computationally heavy, nothing very interesting\n",
    "\n",
    "n=32\n",
    "avg_nnd = pd.DataFrame()\n",
    "for condition in ['SPON647', 'DEP647']:\n",
    "    nnd_array = np.array(storm_data[storm_data['647nm'] == condition]['15_NNs_647'].to_list(), dtype=object)\n",
    "    nnd_array = np.concatenate((nnd_array), axis=None)\n",
    "    nnd_array = nnd_array.reshape((int(nnd_array.shape[0]/(n+1)), n+1))\n",
    "    \n",
    "    avg_nnd[condition] = np.mean(nnd_array, axis=0)\n",
    "    print(avg_nnd[condition].shape)\n",
    "    \n",
    "# Step 2: Plot average nearest neighbor distances for each condition\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(avg_nnd)\n",
    "ax.set_xlabel('Rank')\n",
    "ax.set_ylabel('Average nearest neighbor distance')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(avg_nnd, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce22336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_3d_to_2d(arr):\n",
    "    # Get the maximum value of the array along the z-axis\n",
    "    # Get the sum of the maximum values along the x and y axes\n",
    "    projection = np.sum(arr, axis=2)\n",
    "    \n",
    "    return projection\n",
    "\n",
    "for file_name in list(synapses.keys()):\n",
    "    print(file_name)\n",
    "    im = project_3d_to_2d(synapses[file_name])\n",
    "    plt.imshow(im)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302363d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#VISUALISE\n",
    "\n",
    "for i in list(vesicle_clusters.keys()):\n",
    "    for j in list(vesicle_clusters[i].keys()):\n",
    "        print(i)\n",
    "        print(j)\n",
    "        print(vesicle_clusters[i][j].shape[0])\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "        ax.scatter(vesicle_clusters[i][j][:,0], vesicle_clusters[i][j][:,2], vesicle_clusters[i][j][:,1], c = 'r', marker='o', alpha=0.2)\n",
    "    \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be410940",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_est = {}\n",
    "mean_dens = {}\n",
    "for i in keywords:\n",
    "    print(i)\n",
    "    for j in range(0, len(data_sorted[i])):\n",
    "            data = data_sorted[i][j].T\n",
    "\n",
    "            #determine bounding box for the density estimation of points in the synapse\n",
    "            xmin = data[0].min()\n",
    "            xmax = data[0].max()\n",
    "            ymin = data[1].min()\n",
    "            ymax = data[1].max()\n",
    "            zmin = data[2].min()\n",
    "            zmax = data[2].max()\n",
    "            \n",
    "            #get kde based on data\n",
    "            kde = sc.gaussian_kde(data, 10)\n",
    "            \n",
    "            #define the kernel size in all dimensions\n",
    "            xstep = int((xmin-xmax)/40)\n",
    "            ystep = int((ymin-ymax)/40)\n",
    "            zstep = int((zmin-zmax)/40)\n",
    "            \n",
    "            #projec the kde onto a grid of points within the bounding box given a timestep\n",
    "            X, Y, Z = np.mgrid[xmin:xmax:(xstep*1j), ymin:ymax:(ystep*1j), zmin:zmax:(zstep*1j)]\n",
    "            positions = np.vstack([X.ravel(), Y.ravel(), Z.ravel()])\n",
    "            kde_est = np.reshape(kde(positions).T, X.shape)\n",
    "            mean = np.mean(kde_est)\n",
    "            var = np.std(kde_est)\n",
    "            if i in density_est:\n",
    "                density_est[i] = np.append(density_est[i], kde_est)\n",
    "            else:\n",
    "                density_est[i] = kde_est\n",
    "                \n",
    "            if i in mean_dens:\n",
    "                mean_dens[i] = np.append(mean_dens[i], [mean, var])\n",
    "            else:\n",
    "                mean_dens[i] = [mean, var]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830f2b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=1e-10\n",
    "#list_of_thresholds = list(range(0, threshold, 10))\n",
    "\n",
    "file_dic = min_coloc_dist\n",
    "\n",
    "for key in list(file_dic.keys()):\n",
    "    #data = file_dic[key][(file_dic[key]<threshold)]\n",
    "    data = file_dic\n",
    "    #data = data.flatten()\n",
    "    plt.title(key)\n",
    "    plt.hist(data, bins = 30, label= list(file_dic.keys()), alpha=0.5, density=True)\n",
    "    plt.xlabel('density', fontsize = 18)\n",
    "    plt.ylabel('cumulated area', fontsize = 18)\n",
    "    plt.xticks(fontsize = 14)\n",
    "    plt.yticks(fontsize = 14)\n",
    "plt.show()\n",
    "\n",
    "plt.boxplot(list(mean_dens.values()), showmeans=True, labels=list(file_dic.keys()))\n",
    "plt.xticks(fontsize=6)\n",
    "plt.title(f'average density profile')\n",
    "\n",
    "for i in range(0, len(list(file_dic.keys()))):\n",
    "    stats = sc.describe(file_dic[list(file_dic.keys())[i]])\n",
    "    print(list(file_dic.keys())[i], stats)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca4b0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the nearest neighbor distance and ratio\n",
    "\n",
    "def getArea(points):\n",
    "    \"\"\"returns a list containing the bottom left and the top right \n",
    "    points in the sequence\n",
    "    Here, we use min and max four times over the collection of points\n",
    "    \"\"\"\n",
    "    xmin = points[:,0].min()\n",
    "    xmax = points[:,0].max()\n",
    "    ymin = points[:,1].min()\n",
    "    ymax = points[:,1].max()\n",
    "    zmin = points[:,2].min()\n",
    "    zmax = points[:,2].max()\n",
    "\n",
    "    return (xmax - xmin) * (ymax - ymin) * (zmax - zmin)\n",
    "\n",
    "NND = {}\n",
    "NNR = {}\n",
    "for i in keywords:\n",
    "    for array in data_sorted[i]:\n",
    "        tree = KDTree(array)\n",
    "        nearest_dist, nearest_ind = tree.query(array, k=2) \n",
    "        nearest_dist = nearest_dist[:,-1]\n",
    "        \n",
    "        mean_NND = sum(nearest_dist)/len(array)\n",
    "\n",
    "        area = getArea(array)\n",
    "        exp_mean_NND =  0.5/np.sqrt(len(array)/area)     \n",
    "        NNR_t = mean_NND/exp_mean_NND\n",
    "        \n",
    "        if i in NND:\n",
    "            NND[i] = np.concatenate((NND[i], nearest_dist))\n",
    "        else:\n",
    "            NND[i] = nearest_dist.flatten()\n",
    "            \n",
    "        if i in NNR:\n",
    "            NNR[i] = np.concatenate((NNR[i], [NNR_t]))\n",
    "        else:\n",
    "            NNR[i] = NNR_t.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c77613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(list(NND.keys()))):\n",
    "    plt.title(list(NND.keys())[i])\n",
    "    plt.hist(NND[list(NND.keys())[i]], bins = 20)\n",
    "\n",
    "plt.plot(list(NNR.values()), labels=list(file_dic.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a97929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "random.seed(0)\n",
    "simulation, sim_params = fcts.simulation_batch(5, [17,17,50], [1000,1000,20])\n",
    "\n",
    "peak_coords, sim_clusters, sim_vesicles = fcts.get_clustered_points(simulation[0], params)\n",
    "\n",
    "# Create a 3D scatter plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "peak_coords_plot = peak_coords*params['sf']\n",
    "\n",
    "ax.scatter(simulation[0][:, 0], simulation[0][:, 1], simulation[0][:, 2], marker='o', alpha=0.5)\n",
    "ax.scatter(peak_coords_plot[:, 0], peak_coords_plot[:, 1], peak_coords_plot[:, 2], c='black')\n",
    "\n",
    "# Set axis labels\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "print(np.max(sim_clusters))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc637c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets try pca \n",
    "storm_data['points_norm'] = storm_data.apply(lambda x: x['points']- x['point_center'], axis=1)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "storm_data['PCAs'] = storm_data['points_norm'].apply(lambda x:PCA(n_components=1).fit_transform(x))\n",
    "storm_data['PCAs'] = storm_data['PCAs'].apply(lambda x:x.flatten().tolist())\n",
    "storm_data['PCAs variation'] = storm_data['points_norm'].apply(lambda x:PCA(n_components=1).fit(x).explained_variance_ratio_)\n",
    "storm_data['PCAs variation'] = storm_data['PCAs variation'].apply(lambda x:x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f34695",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1001\n",
    "arr = np.sort(storm_data['PCAs'][i][:,0])\n",
    "plt.scatter(np.linspace(0,10,storm_data['PCAs'][i].shape[1]), arr, s=1)\n",
    "print(storm_data['markertype and DIV'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76f070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "dict_pcapoints = {'DEP647':[], 'SPON647':[]}\n",
    "\n",
    "for i, group in enumerate(storm_data.groupby('647nm')):\n",
    "    for j, row in group[1].iterrows():\n",
    "        for item in row['PCAs']:\n",
    "            dict_pcapoints[group[0]].append(item)\n",
    "\n",
    "#plt.hist(dep_10_r, density=True, alpha=0.5)\n",
    "#plt.hist(dep_8_r, density=True, alpha=0.5)\n",
    "plt.hist(dict_pcapoints['DEP647'], bins=100, density=True, alpha=0.5)\n",
    "plt.hist(dict_pcapoints['SPON647'], bins=100, density=True, alpha=0.5)\n",
    "\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "s_dep = np.std(dict_pcapoints['DEP647'])\n",
    "m_dep = np.mean(dict_pcapoints['DEP647'])\n",
    "\n",
    "s_spon = np.std(dict_pcapoints['SPON647'])\n",
    "m_spon = np.mean(dict_pcapoints['SPON647'])\n",
    "\n",
    "p_spon = norm.pdf(x, m_spon, s_spon)\n",
    "p_dep = norm.pdf(x, m_dep, s_dep)\n",
    "\n",
    "plt.plot(x, p_spon, linewidth=2, color='red')\n",
    "plt.plot(x, p_dep, 'k', linewidth=2, color='blue')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158fa113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_peak_est(data, step):\n",
    "    #determine bounding box for the density estimation of points in the synapse\n",
    "    data = data.T\n",
    "    xmin = data[0].min()\n",
    "    xmax = data[0].max()\n",
    "    ymin = data[1].min()\n",
    "    ymax = data[1].max()\n",
    "    zmin = data[2].min()\n",
    "    zmax = data[2].max()\n",
    "\n",
    "    #get kde based on data\n",
    "    kde = sc.gaussian_kde(data)\n",
    "\n",
    "    #define the kernel size in all dimensions\n",
    "    xstep = int((xmin-xmax)/step)\n",
    "    ystep = int((ymin-ymax)/step)\n",
    "    zstep = int((zmin-zmax)/step)\n",
    "\n",
    "    #projec the kde onto a grid of points within the bounding box given a timestep\n",
    "    X, Y, Z = np.mgrid[xmin:xmax:(xstep*1j), ymin:ymax:(ystep*1j), zmin:zmax:(zstep*1j)]\n",
    "    positions = np.vstack([X.ravel(), Y.ravel(), Z.ravel()])\n",
    "    kde_est = np.reshape(kde(positions).T, X.shape)\n",
    "    \n",
    "    return kde_est\n",
    "\n",
    "storm_data['gaussian_project'] = storm_data.apply(lambda row: gaussian_peak_est(row['points'],30), axis=1)\n",
    "\n",
    "def get_cluster_peaks(wide_field_vesicles, params):\n",
    "    \n",
    "    max_threshold_ves = params['max_threshold_ves']\n",
    "    min_peak_distance = params['min_peak_distance']\n",
    "    \n",
    "    ves_thresh = wide_field_vesicles.mean() + wide_field_vesicles.std() * max_threshold_ves\n",
    "    mask_vesicles = (wide_field_vesicles > ves_thresh) * 1\n",
    "    peak_coords = peak_local_max(wide_field_vesicles, min_distance=min_peak_distance)\n",
    "    print(len(peak_coords))\n",
    "    #if len(peak_coords)!= 0:\n",
    "    #    peak_coords = np.array(fcts.filter_peaks(peak_coords, min_peak_distance))\n",
    "    return peak_coords\n",
    "\n",
    "params['min_peak_distance'] = 15\n",
    "\n",
    "storm_data['cluster_peaks'] = storm_data.apply(lambda row: fcts.get_local_max_3d(row['gaussian_project'],(1,1,1),1), axis=1)\n",
    "storm_data['number_of_peaks'] = storm_data['cluster_peaks'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54ca278",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(storm_data['PCAs'][1])\n",
    "\n",
    "data = storm_data['gaussian_project'][733]\n",
    "peaks = storm_data['cluster_peaks'][733]\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "print(peaks)\n",
    "plt.imshow(data[:,:,3])\n",
    "plt.plot(peaks[:, 1],peaks[:, 0],'rx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbcc6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = '647nm'\n",
    "y = 'number_of_peaks'\n",
    "\n",
    "ttest, pval = sc.ttest_ind(storm_data[storm_data['647nm']=='DEP647']['number_of_peaks'], \n",
    "                           storm_data[storm_data['647nm']=='SPON647']['number_of_peaks'])\n",
    "\n",
    "# Set up plot\n",
    "fig, axes = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "# Volume plot\n",
    "sns.violinplot(data=storm_data.sort_values(by=[x]), x=x, y=y, \n",
    "             linewidth=0, palette='colorblind')\n",
    "sns.stripplot(data=storm_data.sort_values(by=[x]), x=x, y=y, jitter=True, color='black', alpha=0.7, size=2)\n",
    "\n",
    "plt.xlim([-0.5, len(storm_data[x].unique())-0.5])\n",
    "plt.xlabel('labelled vesicle pool')\n",
    "plt.xticks(ticks = [0,1],labels = ['depolarised', 'spontaneous'])\n",
    "\n",
    "for i, group in enumerate(storm_data.groupby(x)):\n",
    "    mean_val = group[1][y].mean()\n",
    "    plt.plot([i-0.2, i+0.2], [mean_val, mean_val], 'k-', lw=2)\n",
    "\n",
    "plt.annotate(f'p-value = {pval:6f}',\n",
    "            xy=(1.4, 880),\n",
    "            horizontalalignment='right', verticalalignment='top')    \n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vesicleSTORM] *",
   "language": "python",
   "name": "conda-env-vesicleSTORM-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
